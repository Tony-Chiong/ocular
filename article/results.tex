\section{Experimental Results}

In order to evaluate our method, we use the BCI Competition IV dataset 2a \citep{brunner2008bci}, which contains 4-class motor imagery EEG data from 9 subjects. Each subject participated in two sessions of 6 runs on different days. The training data is from the first session, and the test data is from the second. A run consist of 48 labeled trials, divided evenly between the 4 classes. Each trial measured the brain signals of a subject on 22 EEG channels and 3 EOG channels. We disregard the EOG channels since we are interested in correcting artifacts without any reference signals. Examples of trials, channels, runs, and sessions are shown in \cref{fig:dataset}.

\begin{figure*}
	\centering
	\begin{adjustbox}{width=\textwidth}
		\includegraphics{figures/bciiv2a.png}
	\end{adjustbox}
	\caption{Sessions, runs, channels, trials, and the relations between them.}
	\label{fig:dataset}
\end{figure*}

We set up two pipelines to be compared. The first consists of OACL, followed by FBCSP and Random Forest. The second is without the OACL step, i.e., just FBCSP and Random Forest. We perform 6-fold cross-validation on the training data using five runs for training and one for validation. We ran 200 iterations of Bayesian Optimization of select hyperparameters for each pipeline. The best hyperparameters for each pipeline is then used for the final evaluation where we try to predict the labels for the test data. We repeat this for each subject. \Cref{fig:results} shows the obtained accuracies and KAPPA scores for each subject.

\begin{table}[H]
	\centering
	\caption{Accuracy and KAPPA score for each subject.}
	\label{fig:results}
	\begin{tabular}{@{}c|cc|cc@{}}
		\toprule
		\textbf{S}             & \multicolumn{2}{c|}{\textbf{No OACL}} & \multicolumn{2}{c}{\textbf{OACL}} \\ \midrule
		\multicolumn{1}{c|}{}  & Acc                   & KAPPA             & Acc                   & KAPPA             \\ \midrule
		1 &                       &                   &                       &                   \\
		2 &                       &                   &                       &                   \\
		3 &                       &                   &                       &                   \\
		4 &                       &                   &                       &                   \\
		5 & \textbf{}             &                   & \textbf{}             &                   \\
		6 &                       &                   &                       &                   \\
		7 &                       &                   &                       &                   \\
		8 &                       &                   &                       &                   \\
		9 &                       &                   &                       &                   \\ \bottomrule
	\end{tabular}
\end{table}

To determine the significance of these results we use the Wilcoxon signed-rank test. \Cref{fig:wilcoxon} shows the results of the test.

\begin{table}[H]
	\centering
	\caption{Wilcoxon signed-rank test}
	\label{fig:wilcoxon}
	\begin{tabular}{@{}l|llll@{}}
		\toprule
		S & No OACL & OACL & Diff & Rank \\ \midrule
		1 &               &                 &      &      \\
		2 &               &                 &      &      \\
		3 &               &                 &      &      \\
		4 &               &                 &      &      \\
		5 &               &                 &      &      \\
		6 &               &                 &      &      \\
		7 &               &                 &      &      \\
		8 &               &                 &      &      \\
		9 &               &                 &      &      \\ \bottomrule
	\end{tabular}
\end{table}

\subsection{Discussion}\label{sec:discussion}
\todo{discuss that optimal filterbank ranges are different from subject to subject}
In the original OACL paper by \citep{li2015ocular}, the relative heights ranges specified in \cref{eq:ranges} were determined by manual inspection of the characteristics of ocular artifacts. Since we generalised this as an optimization of hyperparameter, the found ranges are no longer guaranteed to be optimal in regards to ocular artifacts, but instead optimized for correcting the noise that most negatively affects the classification results. In fact, since we optimize the ranges for maximal classification accuracy, the method may be removing parts of the signal that are technically not artifacts, but removing them increases the performance of the classification model.

Moreover, we have run the Bayesian optimization with the default settings with regards to exploration vs. exploitation. Since running experiments on the pipeline in \cref{fig:ProgramPipeline} is quite expensive, it would be ideal to tune the Bayesian optimization to spend more time on selecting the best candidate future sample to perform the next experiment on.
\todo{skriv at man kunne h√•ndtere residuals evt. ved kald til subroutine in konstruktionen af et artifact signal.}
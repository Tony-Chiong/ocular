\section{Introduction}\label{sec:introduction}
% Relevance. What is the general motivation ---> What is the concrete problem.
The field of Brain-Computer Interfaces (BCI) has in the recent years been under active research, especially with the popularity of machine learning techniques. The reason for the interest, is the many useful application of a well-working BCI, such as replacing lost motor function in disabled people, helping with analysis in brain imaging to diagnose brain conditions or novel applications in computer games. 

The general idea of a BCI is to measure brain activity usually represented by electroencephalogram (EEG) signals, by putting sensors on the scalp that can measure the electric impulses. Each sensor is referred to as a channel in the EEG data. However, the EEG data is noisy at best, and this problem can severely affect the results of classification algorithms. Therefore, artifact removal and feature extraction\todo{What in the aforementioned explains why feature extraction is important?} are important steps in any given BCI.

\cite{uriguen2015eeg} explain that non-physiological artifacts can usually be avoided or trivially removed. The physiological artifacts they have found to be most common are electrooculographic (EOG), electromyographic (EMG), and electrocardiographic (ECG) signals. They are also referred to as ocular, muscle, and cardiac artifacts respectively. Additionally, artifact removal can be done with or without a reference signal such as the EOG, and semi-automatically or automatically (i.e., with or without human intervention). Methods that do not need reference signals are more generally applicable, since reference signals are not always recorded and may be preferable not measure for the comfort of the subjects. Additionally, methods that are automatic are preferred to semi-automatic ones, since the interaction required in the latter usually necessitates the opinion of an expert.

%2. Explain the problem that you study — be clear, do not dive into unnecessary details.
This leaves us with the problem of deciding which techniques should be applied to obtain a corrected EEG signal, and consequently an accurate model that classifies the EEG data. Each technique may require several hyperparameters that need tuning to achieve the optimal results, such as regularization parameters in machine learning algorithms. Such tuning is usually done manually, by experimenting with different values to see their effect on some validation data. Users of the BCI or medical professionals might be knowledgeable about tuning some of the parameters but not all, hence it requires either an expert to help determine them or extensive personnel training. Nonetheless, it often requires a great deal of time tuning the parameters to obtain satisfactory results. Another, more useful approach, would be to automatically infer the hyperparameters from the training data. Such automatic parameter tuning has had positive results through Bayesian optimization \citep{brochu2010tutorial,snoek2012practical,shahriari2016taking} which is an optimization technique that minimizes an unknown objective function by building a Bayesian probabilistic model of the function. The probabilistic model is built by treating the objective function as a black box.

%3  Describe your achievements — you can for example list them.
We propose a technique for correcting artifacts from EEG data based on the method by \citep{li2015ocular}. We adapt the technique for use in multi-class datasets by optimizing hyper-parameters for the artifact correction through Bayesian optimization. This is different from the original approach, where parameters were either manually set or determined by binary logistic regression.

%4  Comment on related work — compare your approach with others; mention both the strengths and weaknesses.
\subsection{Related Work}
% Artifact Removal
Many methods exist for artifact correction in EEG signals. Well-known ones such as \emph{Principal Component Analysis} (PCA), \emph{Independent Component Analysis} (ICA), and \emph{Wavelet Transform} (WT) have had mixed results in the research area \citep{uriguen2015eeg}\todo{Too vague}. Both PCA and ICA are examples of \emph{Blind Source Separation} techniques. PCA is used to reduce the dimensionality of the data, and will return the principal components that maximizes variance of the data. ICA is a way of computing the independent components of a linear combined mixed source. WT can be used to decompose a signal into components describing the frequency over time of a signal. \cite{krishnaveni2006automatic} have used WT to automatically detect and remove ocular artifacts, but the efficiency of their method remains to be tested.

Other approaches such as \emph{Eye Movement Correction Procedure} (EMCP) \citep{gratton1983new} utilises EOG signals measured from the eyes of the subject to detect ocular artifacts and then estimate a propagation factor that can be used to determine the amount of EOG to remove from the EEG signals. \cite{hoffmann2008correction} later did a review of EMCP and ICA, and found that even though these methods reduced the mutual information between the EOG and EEG signals, there were still residual artifacts present up to 250 ms afterwards. \cite{li2015ocular} have had positive results in estimating a pseudo-EOG signal for binary class EEG data, making it possible to obtain an artifact signal without using a reference signal. Key benefits of this approach is that artifacts can be corrected from EEG signals alone, which obviates the difficulty of handling the mutual effects between EEG and EOG signals.

% Feature extraction
The EEG measured on a channel forms a time series where the changes in amplitude over time contains the information that is to be classified. In order to reason about the data, this information must be extracted into features that can be used to train a classifier. Application of ICA, PCA, or WT directly extract components that can be used as feature vectors. Another related technique is the \emph{Common Spatial Patterns} algorithm (CSP). CSP has been successful in extracting features that maximizes the potential for classification in EEG data \citep{ang2008filter,ang2012filter}.

% Classification of EEG
For classification of EEG feature vectors, several algorithms have been successful in achieving good results. The survey by \citet{chan2015systematic} on the performance of ensemble methods in EEG contexts argues that the classification algorithm Random Forests more accurately classifies EEG data than other well-known methods such as k nearest neighbors and Support Vector Machines. \citet{sun2007experimental} also surveys the effectiveness of ensemble methods, but argues that performance is subject to the choice of base classifier as weak learners. 

% Automatic parameter tuning
Several methods for (automatic) hyperparameter optimization exist such as grid search, random search, Bayesian optimization, and gradient-based hyperparameter optimization. Grid search is the most straightforward of the methods, but also the slowest since the approach is to evaluate all combinations of the parameters. Random search does not need to exhaustively try all combinations in the search space, but starts at a random position in the search space and then evaluates new positions until some termination criterion is met. Bayesian optimization uses Gaussian processes to model an objective function with evaluations of the objective function as the posterior. The usefulness of Bayesian optimization is that it is possible to optimize over non-differentiable objective functions, since the objective function is treated as a black box. Gradient-based hyperparameter optimization uses reverse-mode differentiation in order to estimate the gradient of function, which is then possible to optimize using gradient descent.
% Someone proposed to make several passes over eeg to remove a single type of artifact?

%5 Give an overview of the sections to follow
The paper is structured as follows. In \cref{sec:oacl} we explain the artifact correction scheme and how our adaptation differs from the original technique proposed by \citet{li2015ocular}. In \cref{sec:fbcsp} we discuss how to extract features from the corrected EEG time series by the filter-bank multi class Common Spatial Patterns algorithm. In \cref{sec:randomforest} we discuss classification of EEG features by the Random Forest classification algorithm. We then explain how we apply Bayesian optimization to optimize hyperparameters in \cref{sec:bayesian-optimization}. Finally, we evaluate and discuss the results in \cref{sec:results}.
%\subsection{Overview}
%Ocular artifacts such as eye movements or blinking are often present in EEG data, and can be the cause for significant decrease in classification accuracy. The reason for this, is that the amplitude of a signal changes when eye movements happen and can introduce uncertainty about the information hidden in the signal that we are interested in classifying. In order to reduce the impact of these ocular artifacts, we use the OACL technique proposed by \citet{li2015ocular} and generalize it to handle multi-class EEG data. Additionally, we optimize the hyperparameters with Bayesian Optimization (BO). Our approach is illustrated in \cref{fig:ProgramPipeline}. The preprocessing steps consist of correcting artifacts from the data using OACL, then the corrected data is bandpass filtered to create a filterbank, each of the filters is then spatially filtered using CSP with a one-versus-rest approach, and then the spatially filtered data for each sub-band is classified.

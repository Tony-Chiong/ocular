\section{Introduction}
The field of Brain-Computer Interfaces (BCI) has in the recent years been under active research, especially with the popularity of machine learning techniques. The reason for the interest, is the many useful application of a well-working BCI, such as replacing lost motor function in disabled people, helping with analysis in brain imaging to diagnose brain conditions or novel applications in computer games. 

The general idea of a BCI is to measure brain activity usually represented by electroencephalogram (EEG) signals, by putting sensors on the scalp, which can measure the electric impulses. However, the EEG data is noisy at best, and this problem can severely affect the results of classification algorithms. Therefore, signal processing for extracting important components of a signal or removal of artifacts, is an important step in any given BCI.

\cite{uriguen2015eeg} explain that non-physiological artifacts can usually be avoided or trivially removed. The physiological artifacts they have found to be most common are electrooculographic (EOG), electromyographic (EMG), and electrocardiographic (ECG) signals. They are also referred to as ocular, muscle, and cardiac artifacts respectively. Additionally, artifact removal can be done with or without a reference signal such as the EOG, and automatically or semi-automatically. Methods that do not need reference signals are more generally applicable, since reference signals are not always recorded and may be preferable to subjects for their comfort. 

This leaves us with several steps in which several techniques may be applied to obtain a corrected EEG signal and consequently obtain an accurate model that classifies the EEG data. Each technique applied may require several hyperparameters to be tuned for obtaining the optimal results, such as regularization parameters in machine learning algorithms. Such tuning is usually done manually, by experimenting with different values to see their effect on some validation data. Users of the BCI or medical professionals might be knowledgeable about tuning some of the parameters but not all, hence it requires either an expert to help determine them or extensive personnel training. Nonetheless, it requires a great deal of time for tuning the parameters, to obtain the best results.

Another, more useful approach would be to automatically infer the hyperparameters from the training data. Recent work about algorithmically optimizing machine learning parameters has seen popularity by using \emph{Bayesian Optimization} \citep{brochu2010tutorial,snoek2012practical,shahriari2016taking}. Bayesian optimization is an optimization technique that minimizes an unknown objective function by building a bayesian probabilistic model of the function, by sampling input/output. 


\subsection{Related Work}
% Artifact Removal
Much research effort has been put into developing or applying techniques for artifact correction in EEG signals. Well-known methods such as \emph{Principal Component Analysis} (PCA), \emph{Independent Component Analysis} (ICA), and \emph{Wavelet Transform} (WT) have had mixed results in the research area \citep{uriguen2015eeg}. Both PCA and ICA are examples of \emph{Blind Source Separation} techniques. PCA is used to reduce the dimensionality of the data, and will return the principal components that maximizes variance of the data. ICA is a way of computing the independent components of a linear combined mixed source. WT can be used to decompose a signal into components describing the frequency over time of a signal. \cite{krishnaveni2006automatic} have used WT to automatically detect and remove ocular artifacts, but the efficiency of their method remains to be tested. 

Other approaches such as \emph{Eye Movement Correction Procedure} (EMCP) \citep{gratton1983new} utilises EOG signals measured from the eyes of the subject to detect ocular artifacts and then estimate a propagation factor that can be used to determine the amount of EOG to remove from the EEG signals. \cite{hoffmann2008correction} later did a review of EMCP and ICA, and found that even though these methods reduced the mutual information between the EOG and EEG signals, there were still residual artifacts present up to 250 ms afterwards. \cite{li2015ocular} have had positive results in estimating a pseudo-EOG signal for binary class EEG data, making it possible to obtain an artifact signal without using a reference signal as used in EMCP. Key benefits of this approach is that artifacts can be corrected from EEG signals alone which excludes the difficulty of handling the mutual effects between EEG and EOG signals.

% Feature extraction
The EEG measured on a channel forms a time series where the changes in amplitude over time contains the information that is sought to be classified. In order to reason about the data, this information must be extracted into features that can be used to train a classification algorithm. Application of ICA, PCA or DWT directly extract components that can be directly used as feature vectors. Another related techinque, is the \emph{Common Spatial Patterns} algorithm (CSP). CSP have been successful in extracting features that maximizes the potential for classification in EEG data \citep{ang2008filter,ang2012filter}.

% Classification of EEG
For classification of EEG data, several algorithms have been successful in achieving good results. The survey by \citet{chan2015systematic} on the performance of ensemble methods in EEG contexts argues that the classification algorithm Random Forests more accurately classifies EEG data than other well-known methods such as k nearest neighbors and Support Vector Machines. \citet{sun2007experimental} also surveys the effectiveness of ensemble methods, but argues that performance is subject to the choice of base classifier as weak learners. 

% Automatic parameter tuning
Bayesian optimization is described in \cref{sec:bayesian-optimization}.
\todo{sudden ending, might want to include some more. or move BO down to related work and focus on our project in introduction}
Several other methods for hyperparameter optimization exists such as grid search, random search, bayesian optimization and gradient-based hyperparameters optimization. Grid search is the most intuitive of the methods, but also the slowest since it is just trying all combinations of the parameters, this can be very time consuming given that one have a large set og parameters. Random search does not need to exhaustively try all combinations in search space, it starts at a random position in the search space and then tries new positions until some termination criterion is met. Bayesian optimization uses Gaussian processes to model a objective function for which we got prior and by probing posterior, this will be explained further in \cref{sec:oacl} Gradient-based hyperparameter optimization uses Reverse-mode differentiation/backpropagation in order to estimate the gradient of function, which then is possible to optimize using gradient decent.
% Someone proposed to make several passes over eeg to remove a single type of artifact
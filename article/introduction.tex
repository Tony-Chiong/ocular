\section{Introduction}
The field of Brain-Computer Interfaces (BCI) has in the recent years been under active research, especially with the popularity of machine learning techniques. The reason for the interest, is the many useful application of a well-working BCI, such as replacing lost motor function in disabled people, helping with analysis in brain imaging to diagnose brain conditions or novel applications in computer games. 

The general idea of a BCI is to measure brain activity usually represented by electroencephalogram (EEG) signals, by putting sensors on the scalp, which can measure the electric impulses. However, the EEG data is noisy at best, and this problem can severely affect the results of classification algorithms. Therefore, signal processing for extracting important components of a signal or removal of noise, is an important step in any given BCI.

\cite{uriguen2015eeg} explain that non-physiological artifacts can usually be avoided or trivially removed. The physiological artifacts they have found to be most common are electrooculographic (EOG), electromyographic (EMG), and electrocardiographic (ECG) signals. They are also referred to as ocular, muscle, and cardiac artifacts respectively. Additionally, artifact removal can be done with or without a reference signal, and automatically or semi-automatically. Methods that do not need reference signals are more generally applicable, since reference signals are not always recorded.

This leaves us with several steps in which several techniques may be applied to obtain a corrected EEG signal and consequently obtain a model that classifies the EEG data reasonably. Each technique applied may require several parameters to be tuned for obtaining the optimal results, such as the regularization parameter for a Support Vector Machine. Such tuning are usually done manually, by experimenting with different values to see their effect on some validation data. Users of the BCI or medical professionals might be knowledgeable about tuning some parameters but not all, hence it requires either an expert to help determine them or extensive training. Nonetheless, it requires a great deal of time for tuning the parameters, to obtain the best results.

Another, more useful approach would be to automatically infer the hyper-parameters from the training data. Recent work about algorithmically optimizing machine learning parameters has seen popularity by using \emph{Bayesian Optimization} \citep{, brochu2010tutorial,snoek2012practical,shahriari2016taking}. The bayesian optimization maximizes an unknown objective function by building a model of the function by trying candidate samples as input to the objective function, and inferencing on the basis of outputs of experiments (see \cref{sec:bayesian-optimization}).

\subsection{Related Work}\todo{investigate if other ways of automatic parameter optimization exists}
Much research effort has been put into developing or applying techniques for noise/artifact correction in EEG signals. Well-known methods, such as \textit{Principal Component Analysis}, \textit{Independent Component Analysis} or \textit{Discrete Wavelet Transform}, has had mixed results in the research area. Most of these techniques considers how to extract the relevant information, instead of reducing the noise present.
Other approaches such as (EMCP/OCAL) considers removing noise from a correction perspective. \cite{gratton1983new} uses electrooculography (EOG) to detect when eye artifacts occur, and to estimate a propagation factor that can be used to determine the amount of EOG to remove from EEG. \cite{hoffmann2008correction} later did a review of EMCP and ICA, and found that even though these methods reduced the mutual information between the EOG and EEG signals, there were still residual artifacts present up to 250 ms afterwards. \cite{li2015ocular} have had positive results in estimating a pseudo-EOG signal for binary class EEG data, making it possible to obtain an artifact signal without using any secondary measurements. Common to most of the above mentioned methods, are that they tend to use additional steps afterwards in the preprocessing step, these steps consisted an bandpass filter of the EEG data, followed by a method called common spatial patterns[source] that is used to maximize the variance between classes in ...
% Someone proposed to make several passes over eeg to remove a single type of artifact

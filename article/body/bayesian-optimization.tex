\subsection{Bayesian Optimization}\label{sec:bayesianoptimization}
Bayesian Optimization (BO) is a method for finding the extrema of expensive functions. In machine learning, we can see classification algorithms as functions to be optimized over their hyperparameters with respect to model accuracy, and thus Bayesian optimzation can be used to find the combination of hyperparameters that yields the highest performance in classification. \citet{snoek2012practical} shows that Bayesian Optimzation can be applied to existing problems for finding even better parameters than the original authors, as well as outperform even experts at tuning machine learning algorithm parameters.  

%- Motivation

%- Overview
%- Broad overview of the algorithm, what does it do, maybe a drawing.
\subsubsection{BO Overview}
BO sees the objective function as a black box. Generally what it tries to do, it fit a gaussian process (GP) to the unknown objective function, by requesting results at various parameter settings, and eliminating GP samples which does not fit to the solution. The remaining GP samples are then used to form a posterior distribution over the objective function. An acquisition function can now be formed by probing the surrogate function. The next parameter setting for probing the objective function will now be the maximum expected expected result gain from the acquisition function.



%- Gaussian Processes
%- Forklar, lidt mere i dybden, hvad en gaussian process er, hvad kan de bruges til, og hvorfor.

%- Acquisition function
%- Forklar, mere i dybden, hvad er en acquisition function, vores valg.

%- Choosing Candidate Parameter Combinations (Expected Improvement)

%- Combining the posterior with the GP model.